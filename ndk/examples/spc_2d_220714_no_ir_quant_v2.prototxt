layer {
  name : "Placeholder"
  type : "Input"
  top: "Placeholder:0"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 112
      dim: 96
    }
  }
}
layer {
  name : "conv2d_1/Conv2D"
  type : "Convolution"
  bottom: "Placeholder:0"
  top: "conv2d_1/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 32
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 2
    stride_w: 2
    pad_n: 0
    pad_s: 1
    pad_w: 0
    pad_e: 1
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "conv2d_1/Relu6"
  type : "ReLU6"
  bottom: "conv2d_1/BatchNorm/FusedBatchNorm:0"
  top: "conv2d_1/Relu6:0"
}
layer {
  name : "expanded_conv/depthwise/depthwise"
  type : "Convolution"
  bottom: "conv2d_1/Relu6:0"
  top: "expanded_conv/depthwise/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 32
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 2
    stride_w: 2
    pad_n: 0
    pad_s: 1
    pad_w: 0
    pad_e: 1
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 32
  }
}
layer {
  name : "expanded_conv/depthwise/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv/depthwise/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv/depthwise/Relu6:0"
}
layer {
  name : "expanded_conv/project/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv/depthwise/Relu6:0"
  top: "expanded_conv/project/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 16
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_1/expand/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv/project/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_1/expand/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 96
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_1/expand/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_1/expand/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_1/expand/Relu6:0"
}
layer {
  name : "expanded_conv_1/depthwise/depthwise"
  type : "Convolution"
  bottom: "expanded_conv_1/expand/Relu6:0"
  top: "expanded_conv_1/depthwise/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 96
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 2
    stride_w: 2
    pad_n: 0
    pad_s: 1
    pad_w: 0
    pad_e: 1
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 96
  }
}
layer {
  name : "expanded_conv_1/depthwise/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_1/depthwise/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_1/depthwise/Relu6:0"
}
layer {
  name : "expanded_conv_1/project/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_1/depthwise/Relu6:0"
  top: "expanded_conv_1/project/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 32
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_2/expand/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_1/project/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_2/expand/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 192
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_2/expand/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_2/expand/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_2/expand/Relu6:0"
}
layer {
  name : "expanded_conv_2/depthwise/depthwise"
  type : "Convolution"
  bottom: "expanded_conv_2/expand/Relu6:0"
  top: "expanded_conv_2/depthwise/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 192
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 1
    stride_w: 1
    pad_n: 1
    pad_s: 1
    pad_w: 1
    pad_e: 1
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 192
  }
}
layer {
  name : "expanded_conv_2/depthwise/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_2/depthwise/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_2/depthwise/Relu6:0"
}
layer {
  name : "expanded_conv_2/project/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_2/depthwise/Relu6:0"
  top: "expanded_conv_2/project/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 32
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_2/add"
  type : "Eltwise"
  bottom: "expanded_conv_1/project/BatchNorm/FusedBatchNorm:0"
  bottom: "expanded_conv_2/project/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_2/add:0"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name : "expanded_conv_3/expand/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_2/add:0"
  top: "expanded_conv_3/expand/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 192
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_3/expand/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_3/expand/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_3/expand/Relu6:0"
}
layer {
  name : "expanded_conv_3/depthwise/depthwise"
  type : "Convolution"
  bottom: "expanded_conv_3/expand/Relu6:0"
  top: "expanded_conv_3/depthwise/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 192
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 2
    stride_w: 2
    pad_n: 0
    pad_s: 1
    pad_w: 0
    pad_e: 1
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 192
  }
}
layer {
  name : "expanded_conv_3/depthwise/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_3/depthwise/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_3/depthwise/Relu6:0"
}
layer {
  name : "expanded_conv_3/project/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_3/depthwise/Relu6:0"
  top: "expanded_conv_3/project/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 48
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_4/expand/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_3/project/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_4/expand/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 288
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_4/expand/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_4/expand/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_4/expand/Relu6:0"
}
layer {
  name : "expanded_conv_4/depthwise/depthwise"
  type : "Convolution"
  bottom: "expanded_conv_4/expand/Relu6:0"
  top: "expanded_conv_4/depthwise/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 288
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 1
    stride_w: 1
    pad_n: 1
    pad_s: 1
    pad_w: 1
    pad_e: 1
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 288
  }
}
layer {
  name : "expanded_conv_4/depthwise/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_4/depthwise/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_4/depthwise/Relu6:0"
}
layer {
  name : "expanded_conv_4/project/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_4/depthwise/Relu6:0"
  top: "expanded_conv_4/project/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 48
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_4/add"
  type : "Eltwise"
  bottom: "expanded_conv_3/project/BatchNorm/FusedBatchNorm:0"
  bottom: "expanded_conv_4/project/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_4/add:0"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name : "expanded_conv_5/expand/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_4/add:0"
  top: "expanded_conv_5/expand/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 288
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_5/expand/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_5/expand/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_5/expand/Relu6:0"
}
layer {
  name : "expanded_conv_5/depthwise/depthwise"
  type : "Convolution"
  bottom: "expanded_conv_5/expand/Relu6:0"
  top: "expanded_conv_5/depthwise/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 288
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 1
    stride_w: 1
    pad_n: 1
    pad_s: 1
    pad_w: 1
    pad_e: 1
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 288
  }
}
layer {
  name : "expanded_conv_5/depthwise/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_5/depthwise/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_5/depthwise/Relu6:0"
}
layer {
  name : "expanded_conv_5/project/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_5/depthwise/Relu6:0"
  top: "expanded_conv_5/project/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 48
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_5/add"
  type : "Eltwise"
  bottom: "expanded_conv_4/add:0"
  bottom: "expanded_conv_5/project/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_5/add:0"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name : "expanded_conv_6/expand/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_5/add:0"
  top: "expanded_conv_6/expand/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 288
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_6/expand/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_6/expand/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_6/expand/Relu6:0"
}
layer {
  name : "expanded_conv_6/depthwise/depthwise"
  type : "Convolution"
  bottom: "expanded_conv_6/expand/Relu6:0"
  top: "expanded_conv_6/depthwise/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 288
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 1
    stride_w: 1
    pad_n: 1
    pad_s: 1
    pad_w: 1
    pad_e: 1
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 288
  }
}
layer {
  name : "expanded_conv_6/depthwise/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_6/depthwise/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_6/depthwise/Relu6:0"
}
layer {
  name : "expanded_conv_6/project/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_6/depthwise/Relu6:0"
  top: "expanded_conv_6/project/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 48
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_6/add"
  type : "Eltwise"
  bottom: "expanded_conv_5/add:0"
  bottom: "expanded_conv_6/project/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_6/add:0"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name : "expanded_conv_7/expand/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_6/add:0"
  top: "expanded_conv_7/expand/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 288
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_7/expand/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_7/expand/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_7/expand/Relu6:0"
}
layer {
  name : "expanded_conv_7/depthwise/depthwise"
  type : "Convolution"
  bottom: "expanded_conv_7/expand/Relu6:0"
  top: "expanded_conv_7/depthwise/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 288
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 2
    stride_w: 2
    pad_n: 1
    pad_s: 1
    pad_w: 0
    pad_e: 1
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 288
  }
}
layer {
  name : "expanded_conv_7/depthwise/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_7/depthwise/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_7/depthwise/Relu6:0"
}
layer {
  name : "expanded_conv_7/project/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_7/depthwise/Relu6:0"
  top: "expanded_conv_7/project/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 64
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_8/expand/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_7/project/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_8/expand/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 384
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_8/expand/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_8/expand/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_8/expand/Relu6:0"
}
layer {
  name : "expanded_conv_8/depthwise/depthwise"
  type : "Convolution"
  bottom: "expanded_conv_8/expand/Relu6:0"
  top: "expanded_conv_8/depthwise/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 384
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 1
    stride_w: 1
    pad_n: 1
    pad_s: 1
    pad_w: 1
    pad_e: 1
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 384
  }
}
layer {
  name : "expanded_conv_8/depthwise/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_8/depthwise/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_8/depthwise/Relu6:0"
}
layer {
  name : "expanded_conv_8/project/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_8/depthwise/Relu6:0"
  top: "expanded_conv_8/project/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 64
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_8/add"
  type : "Eltwise"
  bottom: "expanded_conv_7/project/BatchNorm/FusedBatchNorm:0"
  bottom: "expanded_conv_8/project/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_8/add:0"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name : "expanded_conv_9/expand/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_8/add:0"
  top: "expanded_conv_9/expand/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 384
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_9/expand/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_9/expand/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_9/expand/Relu6:0"
}
layer {
  name : "expanded_conv_9/depthwise/depthwise"
  type : "Convolution"
  bottom: "expanded_conv_9/expand/Relu6:0"
  top: "expanded_conv_9/depthwise/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 384
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 1
    stride_w: 1
    pad_n: 1
    pad_s: 1
    pad_w: 1
    pad_e: 1
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 384
  }
}
layer {
  name : "expanded_conv_9/depthwise/Relu6"
  type : "ReLU6"
  bottom: "expanded_conv_9/depthwise/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_9/depthwise/Relu6:0"
}
layer {
  name : "expanded_conv_9/project/Conv2D"
  type : "Convolution"
  bottom: "expanded_conv_9/depthwise/Relu6:0"
  top: "expanded_conv_9/project/BatchNorm/FusedBatchNorm:0"
  convolution_param {
    num_output: 64
    kernel_size_h: 1
    kernel_size_w: 1
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    bias_term: True
    dilation_h: 1
    dilation_w: 1
    group: 1
  }
}
layer {
  name : "expanded_conv_9/add"
  type : "Eltwise"
  bottom: "expanded_conv_8/add:0"
  bottom: "expanded_conv_9/project/BatchNorm/FusedBatchNorm:0"
  top: "expanded_conv_9/add:0"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name : "final_depthwise/depthwise"
  type : "Convolution"
  bottom: "expanded_conv_9/add:0"
  top: "final_depthwise/depthwise:0"
  convolution_param {
    num_output: 64
    kernel_size_h: 3
    kernel_size_w: 3
    stride_h: 2
    stride_w: 2
    pad_n: 0
    pad_s: 1
    pad_w: 1
    pad_e: 1
    bias_term: False
    dilation_h: 1
    dilation_w: 1
    group: 64
  }
}
layer {
  name : "global_avg/average_pooling2d/AvgPool_step_0"
  type : "Pooling"
  bottom: "final_depthwise/depthwise:0"
  top: "global_avg/average_pooling2d/AvgPool:0"
  pooling_param {
    kernel_size_h: 2
    kernel_size_w: 2
    stride_h: 1
    stride_w: 1
    pad_n: 0
    pad_s: 0
    pad_w: 0
    pad_e: 0
    dilation_h: 1
    dilation_w: 1
    pool: AVE
  }
}
layer {
  name : "fully_connected/dense"
  type : "InnerProduct"
  bottom: "global_avg/average_pooling2d/AvgPool:0"
  top: "Predictions:0"
  inner_product_param {
    num_output: 2
    bias_term: True
  }
}
